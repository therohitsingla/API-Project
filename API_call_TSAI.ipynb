{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install tsai fastai torch pandas numpy matplotlib seaborn"
      ],
      "metadata": {
        "id": "g9Zz9jFKxZ0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extract Top 4 API**"
      ],
      "metadata": {
        "id": "FEOELTg45Ebc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywy1t7WFtL2s",
        "outputId": "5a2e7937-bc37-4951-93eb-727260fdcafa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Data:\n",
            "  API Code      Time of Call  Unnamed: 2  Unnamed: 3  Unnamed: 4  \\\n",
            "0       A1  01-02-2025 00:00         NaN         NaN         NaN   \n",
            "1       A1  01-02-2025 00:58         NaN         NaN         NaN   \n",
            "2       A1  01-02-2025 01:49         NaN         NaN         NaN   \n",
            "3       A1  01-02-2025 02:01         NaN         NaN         NaN   \n",
            "4       A1  01-02-2025 03:11         NaN         NaN         NaN   \n",
            "\n",
            "                      top 4 API  Unnamed: 6 Unnamed: 7    Unnamed: 8  \n",
            "0  create 5 models for each api         NaN        API  No. of Calls  \n",
            "1                           NaN         NaN         A9          2451  \n",
            "2                           NaN         NaN         A2          2438  \n",
            "3                           NaN         NaN         A7          2410  \n",
            "4                           NaN         NaN         A4          2402  \n",
            "\n",
            "Column Names: Index(['API Code', 'Time of Call', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4',\n",
            "       'top 4 API', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8'],\n",
            "      dtype='object')\n",
            "\n",
            "Top 4 APIs based on the number of calls:\n",
            "API Code\n",
            "A9    2451\n",
            "A2    2438\n",
            "A7    2410\n",
            "A4    2402\n",
            "Name: count, dtype: int64\n",
            "Data for A9 saved to /content/top_api/A9_data.csv\n",
            "Data for A2 saved to /content/top_api/A2_data.csv\n",
            "Data for A7 saved to /content/top_api/A7_data.csv\n",
            "Data for A4 saved to /content/top_api/A4_data.csv\n"
          ]
        }
      ],
      "source": [
        "from tsai.all import *\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/API Call Dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the initial few rows and columns for verification\n",
        "print(\"Initial Data:\")\n",
        "print(df.head())\n",
        "print(\"\\nColumn Names:\", df.columns)\n",
        "\n",
        "# Define the column names\n",
        "api_column = \"API Code\"\n",
        "\n",
        "# Count the number of calls for each API\n",
        "api_call_counts = df[api_column].value_counts().nlargest(4)\n",
        "top_apis = api_call_counts.index\n",
        "\n",
        "print(\"\\nTop 4 APIs based on the number of calls:\")\n",
        "print(api_call_counts)\n",
        "\n",
        "# Create the 'top_api' directory if it doesn't exist\n",
        "top_api_dir = os.path.join(output_directory, \"top_api\")\n",
        "os.makedirs(top_api_dir, exist_ok=True)\n",
        "\n",
        "# Filter and save each top API's data into separate CSV files in the 'top_api' directory\n",
        "for api in top_apis:\n",
        "    output_path = os.path.join(top_api_dir, f\"{api}_data.csv\")  # Changed to include top_api_dir\n",
        "    api_df = df[df[api_column] == api]\n",
        "    api_df.to_csv(output_path, index=False)\n",
        "    print(f\"Data for {api} saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying 5 Models for API files\n",
        "*model_names = ['MLP', 'GRU', 'FCN', 'LSTM', 'TCN']*"
      ],
      "metadata": {
        "id": "tZOIWaEqt7Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tsai.all import *\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Folder path containing the CSV files\n",
        "folder_path = '/content/top_api'\n",
        "\n",
        "# Clean and convert the 'Time of Call' column to datetime\n",
        "def clean_time_column(time_str):\n",
        "    cleaned = time_str.replace('A201', '').strip()\n",
        "    return cleaned\n",
        "\n",
        "# Prepare the data for time series models\n",
        "def prepare_data(df, lookback=5):\n",
        "    time_diffs = df['Time of Call'].diff().dt.total_seconds() / 60  # in minutes\n",
        "    time_diffs = time_diffs.dropna()\n",
        "    scaler = MinMaxScaler()\n",
        "    time_diffs_scaled = scaler.fit_transform(time_diffs.values.reshape(-1, 1))\n",
        "    X, y = [], []\n",
        "    for i in range(len(time_diffs_scaled) - lookback):\n",
        "        X.append(time_diffs_scaled[i:i+lookback].flatten())\n",
        "        y.append(time_diffs_scaled[i+lookback])\n",
        "    return np.array(X), np.array(y), scaler, lookback\n",
        "\n",
        "# Custom PyTorch models\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=50, output_size=1):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size//2, output_size)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=50, output_size=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # If input is 2D, add a sequence dimension\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        # LSTM output\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Take the last time step\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "        return output\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=50, output_size=1):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # If input is 2D, add a sequence dimension\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        # GRU output\n",
        "        gru_out, _ = self.gru(x)\n",
        "\n",
        "        # Take the last time step\n",
        "        output = self.fc(gru_out[:, -1, :])\n",
        "        return output\n",
        "\n",
        "class TCNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, padding=0):\n",
        "        super(TCNBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, dilation=dilation, padding=padding)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bn(self.relu(self.conv1(x)))\n",
        "\n",
        "class TCNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=50, output_size=1):\n",
        "        super(TCNModel, self).__init__()\n",
        "        self.tcn_blocks = nn.Sequential(\n",
        "            TCNBlock(input_size, hidden_size, kernel_size=3, dilation=1, padding=1),\n",
        "            TCNBlock(hidden_size, hidden_size, kernel_size=3, dilation=2, padding=2)\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1).transpose(1, 2)  # Reshape for Conv1d\n",
        "        tcn_out = self.tcn_blocks(x)\n",
        "        output = self.fc(tcn_out.mean(dim=2))\n",
        "        return output\n",
        "\n",
        "class FCNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=50, output_size=1):\n",
        "        super(FCNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, hidden_size, kernel_size=8)\n",
        "        self.conv2 = nn.Conv1d(hidden_size, hidden_size*2, kernel_size=5)\n",
        "        self.conv3 = nn.Conv1d(hidden_size*2, hidden_size, kernel_size=3)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure input has shape [batch_size, channels, sequence_length]\n",
        "        x = x.unsqueeze(1)  # Add channel dimension\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to train and predict with a model\n",
        "def train_and_predict(model, X, y, last_timestamp, scaler):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    for epoch in range(200):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        pred_scaled = model(X[-1].unsqueeze(0))\n",
        "    pred_minutes = float(scaler.inverse_transform(pred_scaled.numpy())[0][0])\n",
        "    next_timestamp = last_timestamp + timedelta(minutes=pred_minutes)\n",
        "    return next_timestamp\n",
        "\n",
        "# Combine predictions from all models\n",
        "def combine_predictions(predictions):\n",
        "    reference_time = min(predictions.values())\n",
        "    minute_predictions = [\n",
        "        (pred - reference_time).total_seconds() / 60\n",
        "        for pred in predictions.values()\n",
        "    ]\n",
        "    avg_minutes = sum(minute_predictions) / len(minute_predictions)\n",
        "    return reference_time + timedelta(minutes=avg_minutes)\n",
        "\n",
        "# Initialize results table\n",
        "results_table = PrettyTable(['File', 'Final Predicted Time'])\n",
        "\n",
        "# Iterate through all CSV files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.csv'):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        df = pd.read_csv(filepath)\n",
        "        df['Time of Call'] = pd.to_datetime(df['Time of Call'].apply(clean_time_column), format='%d-%m-%Y %H:%M')\n",
        "        df = df.sort_values('Time of Call')\n",
        "        X, y, scaler, seq_len = prepare_data(df)\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "        last_timestamp = df['Time of Call'].max()\n",
        "\n",
        "        models = [\n",
        "            MLPModel(X.shape[1]),\n",
        "            # Add other models like GRUModel, LSTMModel, FCNModel, TCNModel here\n",
        "        ]\n",
        "        model_names = ['MLP', 'GRU', 'FCN', 'LSTM', 'TCN']\n",
        "        model_predictions = {}\n",
        "        for model, model_name in zip(models, model_names):\n",
        "            try:\n",
        "                prediction = train_and_predict(model, X_tensor, y_tensor, last_timestamp, scaler)\n",
        "                model_predictions[model_name] = prediction\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {model_name} in {filename}: {e}\")\n",
        "\n",
        "        if model_predictions:\n",
        "            final_prediction = combine_predictions(model_predictions)\n",
        "            results_table.add_row([filename, final_prediction.strftime('%d-%m-%Y %H:%M')])\n",
        "            print(f\"File: {filename}, Final Prediction: {final_prediction.strftime('%d-%m-%Y %H:%M')}\")\n",
        "        else:\n",
        "            print(f\"File: {filename}, No valid predictions made.\")\n",
        "\n",
        "# Print final results table\n",
        "print(\"\\nSummary of Predictions:\")\n",
        "print(results_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HLpUQIH20jL",
        "outputId": "4b58c4f2-2a26-48eb-ff44-c085b2fc2677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: A9_data.csv, Final Prediction: 06-07-2025 10:41\n",
            "File: A4_data.csv, Final Prediction: 06-07-2025 11:51\n",
            "File: A7_data.csv, Final Prediction: 06-07-2025 09:34\n",
            "File: A2_data.csv, Final Prediction: 06-07-2025 11:19\n",
            "\n",
            "Summary of Predictions:\n",
            "+-------------+----------------------+\n",
            "|     File    | Final Predicted Time |\n",
            "+-------------+----------------------+\n",
            "| A9_data.csv |   06-07-2025 10:41   |\n",
            "| A4_data.csv |   06-07-2025 11:51   |\n",
            "| A7_data.csv |   06-07-2025 09:34   |\n",
            "| A2_data.csv |   06-07-2025 11:19   |\n",
            "+-------------+----------------------+\n"
          ]
        }
      ]
    }
  ]
}